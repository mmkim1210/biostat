{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c9a5c2d",
   "metadata": {},
   "source": [
    "# Generalized linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4419b6f",
   "metadata": {},
   "source": [
    "## Q1. Moments of exponential family distributions\n",
    "\n",
    "The exponential family distributions have mean and variance as below:\n",
    "\\begin{eqnarray*}\n",
    "  f(y \\mid \\theta, \\phi) &=& \\exp \\left[ \\frac{y \\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi) \\right] \\\\\n",
    "  \\int f(y \\mid \\theta, \\phi) \\ dy &=& \\int \\exp \\left[ \\frac{y \\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi) \\right] dy \n",
    "  = 1\\\\\n",
    "  \\frac{\\partial}{\\partial \\theta} \\int f(y \\mid \\theta, \\phi) \\ dy &=& \n",
    "  \\int \\frac{\\partial}{\\partial \\theta} \\ f(y \\mid \\theta, \\phi) \\ dy = 0 \\\\\n",
    "  &=& \\int \\frac{y - b'(\\theta)}{a(\\phi)} \\exp \\left[ \\frac{y \\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi) \\right] dy \\\\\n",
    "  \\therefore \\mathbb{E}Y &=& \\mu = b'(\\theta) \\\\\n",
    "  \\frac{\\partial^2}{\\partial \\theta^2} \\int f(y \\mid \\theta, \\phi) \\ dy &=& \n",
    "  \\int \\frac{\\partial^2}{\\partial \\theta^2} \\ f(y \\mid \\theta, \\phi) \\ dy = 0 \\\\\n",
    "  &=& \\int \\frac{(y - b'(\\theta))^2 - b''(\\theta)a(\\phi)}{a(\\phi)^2} \n",
    "  \\exp \\left[ \\frac{y \\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi) \\right] dy \\\\ \n",
    "  &=& \\int \\frac{y^2 - 2 y b'(\\theta) + b'(\\theta)^2 - b''(\\theta)a(\\phi)}{a(\\phi)^2} \n",
    "  \\exp \\left[ \\frac{y \\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi) \\right] dy \\\\ \n",
    "  \\mathbb{E}Y^2 &=& b'(\\theta)^2 + b''(\\theta)a(\\phi) \\\\\n",
    "  \\therefore \\operatorname{Var}Y &=& \\sigma^2 = b''(\\theta) a(\\phi)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "## Q2. Score and information matrix of GLM\n",
    "\n",
    "The gradient (score), negative Hessian, and Fisher information matrix (expected negative Hessian) of GLM are as below:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "  \\ell(\\boldsymbol{\\beta}) &=& \\sum_{i=1}^n \\frac{y_i \\theta_i - b(\\theta_i)}{a(\\phi)} + c(y_i, \\phi) \\\\\n",
    "  \n",
    "  \\mu_i &=& b'(\\theta_i) \\\\\n",
    "\n",
    "  \\sigma_i^2 &=& b''(\\theta_i) a(\\phi) \\\\\n",
    "  \n",
    "  \\eta_i &=& \\mathbf{x}_i^T \\beta = g(\\mu_i) \\\\\n",
    "  \n",
    "  \\frac{\\partial \\mu_i}{\\partial \\theta_i} &=& b''(\\theta_i) \\\\\n",
    "  \n",
    "  \\frac{\\partial}{\\partial \\beta_j} &=& \\frac{\\partial}{\\partial \\theta_i} \n",
    "  \\frac{\\partial \\theta_i}{\\partial \\mu_i} \\frac{\\partial \\mu_i}{\\partial \\eta_i} \n",
    "  \\frac{\\partial \\eta_i}{\\partial \\beta_j} \\\\\n",
    "  \n",
    "  \\therefore \\nabla \\ell(\\boldsymbol{\\beta}) &=& \\sum_{i=1}^n \\frac{(y_i - \\mu_i)}{a(\\phi)} \n",
    "  \\frac{1}{b''(\\theta_i)} \\mu_i'(\\eta_i) \\mathbf{x}_i = \n",
    "  \\sum_{i=1}^n \\frac{(y_i - \\mu_i) \\mu_i'(\\eta_i)}{\\sigma_i^2} \\mathbf{x}_i \\\\\n",
    "  \n",
    "  - \\nabla^2 \\ell(\\boldsymbol{\\beta}) &=& \\sum_{i=1}^n \\frac{[\\mu_i'(\\eta_i)]^2}{\\sigma_i^2} \\mathbf{x}_i\n",
    "  \\mathbf{x}_i^T - \\sum_{i=1}^n \\frac{(y_i - \\mu_i) \\mu_i''(\\eta_i)}{\\sigma_i^2} \\mathbf{x}_i \\mathbf{x}_i^T \\\\\n",
    "  & & + \\sum_{i=1}^n \\frac{(y_i - \\mu_i) [\\mu_i'(\\eta_i)]^2 (d \\sigma_i^{2} / d\\mu_i)}{\\sigma_i^4} \\mathbf{x}_i\n",
    "  \\mathbf{x}_i^T\n",
    "\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Of note, it is trivial to show the last equality above (for Hessian matrix) by the chain rule. \n",
    "\n",
    "## Q3. ELMR Exercise 8.1 (p171)\n",
    "\n",
    "### Q3.a \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "  f(y \\mid \\theta, \\phi) &=& \\exp \\left[ \\frac{y \\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi) \\right] \\\\\n",
    "  &=& \\lambda \\exp(-\\lambda y) = \\exp(-\\lambda y + \\log \\lambda) \\\\\n",
    "  \n",
    "  \\therefore \\theta &=& -\\lambda, \\phi = 1, a(\\phi) = 1, b(\\theta) = -\\log(-\\theta), c(y, \\phi) = 0\n",
    "\n",
    "\\end{eqnarray*}\n",
    "\n",
    "### Q3.b\n",
    "The canonical link and variance function are $-\\lambda = -\\frac{1}{\\mu}$ and $\\mu^2$, respectively.\n",
    "\n",
    "### Q3.c\n",
    "The canonical link can only take the negative values, since $\\lambda$ is greater than equal to zero, which is in contrast to the systematic component that can take any value. \n",
    "\n",
    "### Q3.d\n",
    "We would use $\\chi^2$ test when comparing nested models here, as $\\phi = 1$ is known. \n",
    "\n",
    "### Q3.e\n",
    "$$\n",
    "  D(\\mathbf{y}, \\widehat{\\boldsymbol{\\mu}}) = \n",
    "  2 \\sum_i [y_i(\\tilde \\theta_i - \\hat \\theta_i) - b(\\tilde \\theta_i) + b(\\hat \\theta_i)] = \n",
    "  2 \\sum_i (\\frac{y_i - \\hat{u_i}}{\\hat{u_i}} - \\log \\frac{y_i}{\\hat{u_i}})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c44d77",
   "metadata": {},
   "source": [
    "## Q9. (10 pts) GLM\n",
    "\n",
    "You can use the formulae in lecture notes or homework. \n",
    "\n",
    "1. For a GLM with canonical link, explain why the log-likelihood is concave. \n",
    "\\begin{eqnarray*}\n",
    "-\\nabla^2 \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\frac{[\\mu_i'(\\eta_i)]^2}{\\sigma_i^2} \\mathbf{x}_i \\mathbf{x}_i^T\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Hessian matrix of negative log-likelihood is positive semidefinite, so it is concave. \n",
    "\n",
    "2. For a GLM with canonical link, explain why the Fisher scoring algorithm is the same as the Newton-Raphson algorithm. \n",
    "\n",
    "Because expected Fisherian Information is the same as negative Hessian matrix. \n",
    "\n",
    "3. Use Poisson regression (with log link) as example. Show that the Fisher scoring algorithm is equivalent to the IRWLS (iteratively reweighted least squares) procedure. Clarify what are the weights and working responses in this case (Poisson regression with canonical link). \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "- \\nabla^2\\ell(\\boldsymbol{\\beta}) &=& \\sum_{i=1}^n \\mu_i \\mathbf{x}_i \\mathbf{x}_i^T = \\mathbf{X}^T \\mathbf{W} \\mathbf{X}, \\quad \\mathbf{W} = \\text{diag}(w_1, \\ldots, w_n), w_i = \\mu_i \\\\\n",
    "\\mathbf{z}^{(t)} &=& \\mathbf{X} \\boldsymbol{\\beta}^{(t)} + s (\\mathbf{W}^{(t)})^{-1} (\\mathbf{y} - \\widehat{\\boldsymbol{\\mu}}^{(t)})\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    "## Q10. (6 pts) Link functions\n",
    "\n",
    "Write down the (1) names, (2) expressions, and (3) the name of corresponding latent variable distribution of 3 commonly used link functions for a Bernoulli or binomial parameter $p$. \n",
    "\n",
    "Example: Identiy link, $\\eta = g(p) = p$, corresponds to a uniform distribution for the latent variable.\n",
    "\n",
    "Three commonly used link functions for a Bernoulli or binomial parameter are logit, probit, and complementary log-log. \n",
    "\n",
    "$$\n",
    "\\eta = g(p) = \\log \\frac{p}{1-p}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\eta = g(p) = \\Phi^{-1}(p)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\eta = g(p) = \\log ( - \\log(1-p))\n",
    "$$\n",
    "\n",
    "\n",
    "## Q11. (10 pts) Inverse Gaussian\n",
    "\n",
    "The inverse Gaussian distribution $IG(\\mu, \\lambda)$ has density \n",
    "$$\n",
    "f(y) = \\left( \\frac{\\lambda}{2 \\pi y^3} \\right)^{1/2} e^{- \\frac{\\lambda (y - \\mu)^2}{2 \\mu^2 y}}, \\quad y, \\mu, \\lambda > 0.\n",
    "$$\n",
    "\n",
    "1. Show that $IG(\\mu, \\lambda)$ belongs to the exponential family distributions.\n",
    "\\begin{eqnarray*}\n",
    "  f(y \\mid \\theta, \\phi) &=& \\exp \\left[ \\frac{y \\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi) \\right] \\\\\n",
    "  &=& \\exp(-\\frac{y}{2 \\mu^2} + \\frac{1}{\\mu} - \\frac{1}{2}(\\frac{1}{y} - \\log \\frac{\\lambda}{2 \\pi y^3}))\n",
    "\\end{eqnarray*}\n",
    "\n",
    "2. What is the canonical parameters? \n",
    "$-\\frac{1}{2 \\mu^2}$\n",
    "\n",
    "3. Derive the mean and variance of inverse Gaussian.\n",
    "\n",
    "4. What is the canonical link function for inverse Gaussian? \n",
    "\n",
    "5. Derive the deviance formula for $IG(\\mu, \\lambda)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3727edff",
   "metadata": {},
   "source": [
    "## Q1. Concavity of logistic regression log-likelihood \n",
    "\n",
    "### Q1.1\n",
    "\n",
    "From the lecture note, we have previously seen that the log-likelihood function of logistic regression looks as below.\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\ell(\\boldsymbol{\\beta}) &=& \\sum_{i=1}^n \\left[ y_i \\log p_i + (m_i - y_i) \\log (1 - p_i) + \\log \\binom{m_i}{y_i} \\right] \\\\\n",
    "&=& \\sum_{i=1}^n \\left[ y_i \\eta_i - m_i \\log ( 1 + e^{\\eta_i}) + \\log \\binom{m_i}{y_i} \\right] \\\\\n",
    "&=& \\sum_{i=1}^n \\left[ y_i \\cdot \\mathbf{x}_i^T \\boldsymbol{\\beta} - m_i \\log ( 1 + e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}}) + \\log \\binom{m_i}{y_i} \\right].\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    "### Q1.2\n",
    "\n",
    "Then the gradient vector of the log-likelhood function is as below assuming that there are _q_ parameters being estimated. \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\nabla\\ell(\\boldsymbol{\\beta}) = \\begin{pmatrix} \\sum_{i=1}^n y_i \\cdot x_{i1} - m_i \\frac{e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}} \\cdot x_{i1}}{1+e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}}} \\\\\n",
    "\\vdots \\\\\n",
    "\\sum_{i=1}^n y_i \\cdot x_{iq} - m_i \\frac{e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}} \\cdot x_{iq}}{1+e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}}}\n",
    "\\end{pmatrix} = \\begin{pmatrix} \\sum_{i=1}^n x_{i1} (y_i - m_i \\frac{e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}}}{1+e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}}}) \\\\\n",
    "\\vdots \\\\\n",
    "\\sum_{i=1}^n x_{iq} (y_i - m_i \\frac{e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}}}{1+e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}}})\n",
    "\\end{pmatrix}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "The Hessian matrix of the log-likelihood function is the Jacobian of gradient vector above.\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\nabla^2\\ell(\\boldsymbol{\\beta}) = \\begin{pmatrix} \\sum_{i=1}^n -x_{i1}^2 \\cdot m_i \\frac{e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}}}{(1+e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}})^2} & \\cdots & \\sum_{i=1}^n -x_{i1} \\cdot x_{iq} \\cdot m_i \\frac{e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}}}{(1+e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}})^2}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\sum_{i=1}^n -x_{i1} \\cdot x_{iq} \\cdot m_i \\frac{e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}}}{(1+e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}})^2} & \\cdots & \\sum_{i=1}^n -x_{iq}^2 \\cdot m_i \\frac{e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}}}{(1+e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}})^2}\n",
    "\\end{pmatrix}\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e3ea40",
   "metadata": {},
   "source": [
    "A positive self-definite matrix by definition satisfies $\\boldsymbol{w}'H\\boldsymbol{w} \\ge 0$ for all $\\boldsymbol{w} \\in \\mathbb{R}^{qx1}$. If we assume $\\boldsymbol{w} = (w_1 + w_2 + \\cdots + w_q)'$, and perform the above matrix multiplication after taking the negative Hessian, we can complete the squares for given $i$, as $\\left[ m_i \\frac{e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}}}{(1+e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}})^2} \\sum_{j=1}^q (x_{ij} \\cdot w_j)\\right]^2$. This form is obviously greater than or equal to zero, and hence after expanding to other $i$'s, we show that the negative Hessian is a positive semidefinite matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e4f5f2",
   "metadata": {},
   "source": [
    "## Q1. Beta-Binomial \n",
    "\n",
    "### Q1.1\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\pi &\\sim& \\text{Be}(\\alpha, \\beta), \\quad \\pi \\in [0, 1], \\alpha > 0, \\beta > 0 \\\\\n",
    "\\mathbf{E}[\\pi] = \\int_{0}^{1} \\pi \\cdot f(\\pi; \\alpha, \\beta) \\; d\\pi &=& \\int_{0}^{1} \\pi \\cdot \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\pi^{\\alpha - 1} (1 - \\pi)^{\\beta - 1} \\; d\\pi \\\\ \n",
    "&=& \\int_{0}^{1} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\pi^{\\alpha} (1 - \\pi)^{\\beta - 1} \\; d\\pi \\\\\n",
    "&=& \\frac{\\Gamma(\\alpha + \\beta) \\Gamma(\\alpha + 1)}{\\Gamma(\\alpha) \\Gamma(\\alpha + \\beta + 1)} = \\frac{\\alpha}{\\alpha + \\beta} \\\\\n",
    "\\mathbf{E}[\\pi^2] = \\int_{0}^{1} \\pi^2 \\cdot f(\\pi; \\alpha, \\beta) \\; d\\pi &=& \\int_{0}^{1} \\pi^2 \\cdot \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\pi^{\\alpha - 1} (1 - \\pi)^{\\beta - 1} \\; d\\pi \\\\ \n",
    "&=& \\int_{0}^{1} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\pi^{\\alpha + 1} (1 - \\pi)^{\\beta - 1} \\; d\\pi \\\\\n",
    "&=& \\frac{\\Gamma(\\alpha + \\beta) \\Gamma(\\alpha + 2)}{\\Gamma(\\alpha) \\Gamma(\\alpha + \\beta + 2)} = \\frac{(\\alpha + 1) \\alpha}{(\\alpha + \\beta + 1)(\\alpha + \\beta)} \\\\\n",
    "\\mathbf{E}[\\pi^2] - \\mathbf{E}[\\pi]^2 &=& \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "### Q1.2\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "Y_i &\\sim& \\text{Bin}(n_i, \\pi_i) \\\\\n",
    "f(Y_i; \\alpha, \\beta, n_i) &=& \\int_{0}^{1} \\binom{n_i}{y_i} \\pi^{y_i} (1 - \\pi)^{n_i - y_i} f(\\pi; \\alpha, \\beta) \\; d\\pi \\\\\n",
    "&=& \\int_{0}^{1} \\binom{n_i}{y_i} \\pi^{y_i} (1 - \\pi)^{n_i - y_i} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\pi^{\\alpha - 1} (1 - \\pi)^{\\beta - 1} \\; d\\pi \\\\\n",
    "&=& \\int_{0}^{1} \\binom{n_i}{y_i} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\pi^{y_i + \\alpha - 1} (1 - \\pi)^{n_i - y_i + \\beta - 1} \\; d\\pi \\\\\n",
    "&=& \\binom{n_i}{y_i} \\frac{\\Gamma(\\alpha + \\beta) \\Gamma(y_i + \\alpha) \\Gamma(n_i - y_i + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta) \\Gamma(n_i + \\alpha + \\beta)}\\\\\n",
    "\\mathbf{E}[Y_i] &=& \\frac{n_i \\alpha}{\\alpha + \\beta} \\\\\n",
    "\\mathbf{Var}[Y_i] &=& \\frac{n_i \\alpha \\beta (\\alpha + \\beta + n_i)}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Mean and variance of $Y_i$ can be calculated using the marginal probability mass function above $f(Y_i; \\alpha, \\beta, n_i)$. Variance of a Binomial random variable with the same batch size ($n_i$) and mean (i.e. $p = \\frac{\\alpha}{\\alpha + \\beta}$ is $n_i \\cdot p(1-p) = n_i * \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2}$, and as a result is always equal to or less than the variance of $Y_i$ as below. \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\frac{n_i \\alpha \\beta (\\alpha + \\beta + n_i)}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}}{\\frac{n_i \\alpha \\beta}{(\\alpha + \\beta)^2}} = \\frac{\\alpha + \\beta + n_i}{\\alpha + \\beta + 1} \\ge 1\n",
    "\\end{eqnarray*}\n",
    "\n",
    "## Q2. Motivation for quasi-binomial\n",
    "\n",
    "The log-likilihood $\\ell_i$ of a binomial proportion $Y_i$, where $m_i Y_i \\sim \\text{Bin}(m_i, p_i)$, satisfies  \n",
    "\\begin{eqnarray*}\n",
    "\n",
    "\\ell_i &=& m_i y_i \\log p_i + m_i (1 - y_i) \\log (1 - p_i) + \\log \\binom{m_i}{m_i y_i} \\\\\n",
    "\n",
    "\\mathbb{E} \\frac{\\partial \\ell_i}{\\partial \\mu_i} &=& \\mathbb{E} \\frac{m_i y_i}{p_i} - \\frac{m_i(1 - y_i)}{1 - p_i} = \\mathbb{E} \\frac{m_i y_i - m_i p_i}{p_i (1 - p_i)} = 0 \\\\\n",
    "\\operatorname{Var} \\frac{\\partial \\ell_i}{\\partial \\mu_i} &=& \\operatorname{Var} \\frac{m_i y_i - m_i p_i}{p_i (1 - p_i)} =  \\operatorname{Var} \\frac{m_i y_i}{p_i (1 - p_i)} = \\left[ \\frac{m_i}{p_i (1 - p_i)} \\right]^2 \\frac{p_i (1 - p_i)}{m_i} = \\frac{m_i}{p_i (1 - p_i)} = \\frac{1}{\\phi V(\\mu_i)} \\\\\n",
    "\n",
    "\\mathbb{E} \\frac{\\partial \\ell_i^2}{\\partial^2 \\mu_i} &=& = \\mathbb{E} \\frac{- m_i p_i (1 - p_i) - m_i (y_i - p_i) (1 - 2 p_i)}{p_i^2 (1 - p_i)^2} = \\frac{- m_i}{p_i (1 - p_i)} = - \\frac{1}{\\phi V(\\mu_i)}\n",
    "\n",
    "\\end{eqnarray*}\n",
    "\n",
    "where $\\phi = 1$, $\\mu_i = p_i$, and $V(\\mu_i) = p_i (1 - p_i)/m_i$. Therefore, the $U_i$ in quasi-binomial method mimics the behavior of a binomial model.\n",
    "\n",
    "## Q3. Concavity of Poisson regression log-likelihood \n",
    "\n",
    "### Q3.1\n",
    "\n",
    "Let $Y_1,\\ldots,Y_n$ be independent random variables with $Y_i \\sim \\text{Poisson}(\\mu_i)$ and $\\log \\mu_i = \\mathbf{x}_i^T \\boldsymbol{\\beta}$, $i = 1,\\ldots,n$. Then the log-likelihood function is as below. \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\ell(\\boldsymbol{\\beta}) &=& \\sum_i y_i \\log \\mu_i - \\mu_i - \\log y_i! \\\\\n",
    "&=& \\sum_i y_i \\cdot \\mathbf{x}_i^T \\boldsymbol{\\beta} - e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}} - \\log y_i!\n",
    "\\end{eqnarray*}\n",
    "\n",
    "### Q3.2\n",
    "\n",
    "Then the gradient vector and Hessian matrix of the log-likelhood function is as below assuming that there are _q_ parameters being estimated.\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\nabla\\ell(\\boldsymbol{\\beta}) = \\begin{pmatrix} \\sum_{i=1}^n y_i \\cdot x_{i1} - e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}} \\cdot x_{i1} \\\\\n",
    "\\vdots \\\\\n",
    "\\sum_{i=1}^n y_i \\cdot x_{iq} - e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}} \\cdot x_{iq}\n",
    "\\end{pmatrix} = \\begin{pmatrix} \\sum_{i=1}^n x_{i1} (y_i - e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}}) \\\\\n",
    "\\vdots \\\\\n",
    "\\sum_{i=1}^n x_{iq} (y_i - e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}})\n",
    "\\end{pmatrix} \\\\ \\\\\n",
    "\n",
    "\\nabla^2\\ell(\\boldsymbol{\\beta}) = \\begin{pmatrix} \\sum_{i=1}^n -x_{i1}^2 e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}} & \\cdots & \\sum_{i=1}^n -x_{i1} x_{iq} e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\sum_{i=1}^n -x_{i1} x_{iq} e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}} & \\cdots & \\sum_{i=1}^n -x_{iq}^2 e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}}\n",
    "\\end{pmatrix}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    "### Q3.3\n",
    "\n",
    "A positive self-definite matrix by definition satisfies $\\boldsymbol{w}'H\\boldsymbol{w} \\ge 0$ for all $\\boldsymbol{w} \\in \\mathbb{R}^{qx1}$. If we assume $\\boldsymbol{w} = (w_1 + w_2 + \\cdots + w_q)'$, and perform the above matrix multiplication after taking the negative Hessian, we can complete the squares for given $i$, as $e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}} \\left[  \\sum_{j=1}^q (x_{ij} \\cdot w_j)\\right]^2$. This form is obviously greater than or equal to zero, and hence after expanding to other $i$'s, we show that the negative Hessian is a positive semidefinite matrix. \n",
    "\n",
    "### Q3.4\n",
    "\n",
    "Assuming that $\\beta_1$ is the intercept term, then the fitted values $\\widehat{\\mu}_i = e^{\\mathbf{x}_i^T \\boldsymbol{\\widehat{\\beta}}}$ from maximum likelihood estimates satisfies\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\n",
    "\\nabla\\ell(\\boldsymbol{\\widehat{\\beta}}) &=& 0 \\\\\n",
    "\\sum_{i=1}^n x_{i1} (y_i - e^{\\mathbf{x}_i^T \\boldsymbol{\\widehat{\\beta}}}) &=& \n",
    "\\sum_{i=1}^n x_{i1} (y_i - \\widehat{\\mu}_i) = \n",
    "\\sum_{i=1}^n 1 \\cdot (y_i - \\widehat{\\mu}_i) = 0\n",
    "\n",
    "\\end{eqnarray*}\n",
    "\n",
    "## Q4. Odds ratios\n",
    "\n",
    "### Q4.1\n",
    "\n",
    "For the simple logistic model, if there is no difference between exposed and non-exposed groups (i.e. $\\beta_1 = \\beta_2$), then\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\pi_1 &=& \\frac{e^{\\beta_1}}{1 + e^{\\beta_1}} = \\pi_2 = \\frac{e^{\\beta_2}}{1 + e^{\\beta_2}} \\\\ \\\\\n",
    "\\phi &=& \\frac{O_1}{O_2} = \\frac{\\pi_1(1 - \\pi_2)}{\\pi_2 (1 - \\pi_1)} = \\frac{\\pi_1(1 - \\pi_1)}{\\pi_1 (1 - \\pi_1)} = 1\n",
    "\\end{eqnarray*}\n",
    "\n",
    "### Q4.2 \n",
    "\n",
    "Assume that there are $J$ $2 \\times 2$ tables, one for each level $x_j$ of a factor, such as age group, with $j=1,\\ldots, J$. We further assume the logistic model as below,\n",
    "$$\n",
    "\\pi_{ij} = \\frac{e^{\\alpha_i + \\beta_i x_j}}{1 + e^{\\alpha_i + \\beta_i x_j}}, \\quad i = 1,2, \\quad j= 1,\\ldots, J.\n",
    "$$\n",
    "Then for arbitrary $j$, if $\\beta_1 = \\beta_2$,\n",
    "\\begin{eqnarray*}\n",
    "\\log \\phi &=& \\log \\pi_1 + \\log (1 - \\pi_1) - \\log \\pi_2 - \\log (1 - \\pi_1) \\\\\n",
    "&=& \\alpha_1 + \\beta_1 x_j - (\\alpha_2 + \\beta_2 x_j) = \\alpha_1 - \\alpha_2\n",
    "\\end{eqnarray*}\n",
    "\n",
    "As such, there is no $j$ term and $\\log \\phi$ is constant over all tables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
