{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "031b63c7",
   "metadata": {},
   "source": [
    "# Balanced one-way ANOVA random effects model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadae4a9",
   "metadata": {},
   "source": [
    "Consider the balanced one-way ANOVA random effects model with $a$ levels and $n$ observations in each level\n",
    "$$\n",
    "y_{ij} = \\mu + \\alpha_i + \\epsilon_{ij}, \\quad i=1,\\ldots,a, \\quad j=1,\\ldots,n.\n",
    "$$\n",
    "where $\\alpha_i$ are iid from $N(0,\\sigma_\\alpha^2)$, $\\epsilon_{ij}$ are iid from $N(0, \\sigma_\\epsilon^2)$. Here, we estimate the variance components in three different ways. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1ab87b",
   "metadata": {},
   "source": [
    "### ANOVA estimate\n",
    "\\begin{eqnarray*}\n",
    "  \\mathbb{E}(\\bar y_{\\cdot \\cdot}) &=& \\mathbb{E} \\left( \\frac{\\sum_{ij} y_{ij}}{na} \\right) = \\mathbb{E}(y_{ij}) = \\mu \\\\\n",
    "  \\mathbb{E} (\\text{SSE}) &=& \\mathbb{E} \\left[ \\sum_{i=1}^a \\sum_{j=1}^n (y_{ij} - \\bar{y}_{i \\cdot})^2 \\right] = \\sum_{i=1}^a \\mathbb{E} \\left[ \\sum_{j=1}^n (y_{ij} - \\bar{y}_{i \\cdot})^2 \\right] = \\sum_{i=1}^a \\mathbb{E} \\left[ \\left(\\sum_{j=1}^n y_{ij}^2 \\right) - n \\bar{y}_{i \\cdot}^2 \\right] \\\\\n",
    "  &=& \\sum_{i=1}^a \\left[ \\left(\\sum_{j=1}^n \\mathbb{E}[y_{ij}^2] \\right) - n \\mathbb{E}[\\bar{y}_{i \\cdot}^2] \\right] = \\sum_{i=1}^a \\left[ \\left( \\sum_{j=1}^n \\left(\\text{Var}[y_{ij}] + \\mathbb{E}[y_{ij}]^2 \\right) \\right) - n (\\text{Var}[\\bar{y}_{i \\cdot}] + \\mathbb{E}[\\bar{y}_{i \\cdot}]^2) \\right] \\\\\n",
    "  &=& \\sum_{i=1}^a \\left(\\sum_{j=1}^n \\sigma_{\\alpha}^2 + \\sigma_{\\epsilon}^2 + \\mu^2 \\right) - n \\left[\\frac{1}{n^2} (n \\sigma_{\\alpha}^2 + n \\sigma_{\\epsilon}^2 + n(n-1) \\sigma_{\\alpha}^2) + \\mu^2 \\right] \\\\\n",
    "  &=& \\sum_{i=1}^a (n-1)\\sigma_{\\epsilon}^2 = a(n-1) \\sigma_{\\epsilon}^2 \\\\\n",
    "  \\mathbb{E} (\\text{SSA}) &=& \\mathbb{E} \\left[ \\sum_{i=1}^a \\sum_{j=1}^n (\\bar{y}_{i \\cdot} - \\bar{y}_{\\cdot \\cdot})^2 \\right] = \\sum_{i=1}^a \\sum_{j=1}^n \\mathbb{E} \\left[\\bar{y}_{i \\cdot}^2 - 2\\bar{y}_{i \\cdot}\\bar{y}_{\\cdot \\cdot} + \\bar{y}_{\\cdot \\cdot}^2 \\right] \\\\\n",
    "  &=& \\sum_{i=1}^a \\sum_{j=1}^n \\text{Var}[\\bar{y}_{i \\cdot}] + \\mathbb{E}[\\bar{y}_{i \\cdot}]^2 - 2\\mathbb{E}[\\bar{y}_{i \\cdot} \\bar{y}_{\\cdot \\cdot}] + \\text{Var}[\\bar{y}_{i \\cdot \\cdot}] + \\mathbb{E}[\\bar{y}_{i \\cdot \\cdot}]^2\\\\\n",
    "  &=& \\sum_{i=1}^a \\sum_{j=1}^n \\frac{1}{n^2} (n \\sigma_{\\alpha}^2 + n \\sigma_{\\epsilon}^2 + n(n-1) \\sigma_{\\alpha}^2) + \\mu^2 - 2\\frac{1}{n^2 a} (n^2 a \\mu^2 + n^2 \\sigma_{\\alpha} + n \\sigma_{\\epsilon}) + \\frac{1}{(na)^2} (na \\sigma_{\\alpha}^2 + na \\sigma_{\\epsilon}^2 + n(n-1)a \\sigma_{\\alpha}^2) + \\mu^2 \\\\ \n",
    "  &=& \\sum_{i=1}^a \\sum_{j=1}^n (1 - \\frac{1}{a})\\sigma_{\\alpha}^2 + (\\frac{1}{n} - \\frac{1}{na})\\sigma_{\\epsilon}^2 \\\\\n",
    "  &=& (a-1)(n \\sigma_{\\alpha}^2 + \\sigma_{\\epsilon}^2),\n",
    "\\end{eqnarray*}\n",
    "which can be solved to obtain ANOVA estimate\n",
    "\\begin{eqnarray*}\n",
    "\\hat{\\mu} &=& \\frac{\\sum_{ij} y_{ij}}{na}, \\\\\n",
    "\\widehat{\\sigma}_{\\epsilon}^2 &=& \\frac{\\text{SSE}}{a(n-1)}, \\\\\n",
    "\\widehat{\\sigma}_{\\alpha}^2 &=& \\frac{\\text{SSA}/(a-1) - \\widehat{\\sigma}_{\\epsilon}^2}{n}.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "We can also derive this using matrix notation. Let $\\mathbf{P}$ be the orthogonal projection matrix in one-way ANOVA fixed effects setting and $\\mathbf{J} = \\frac{1}{na}\\mathbf{1}_{na}\\mathbf{1}_{na}^T$. Then $\\mathbf{P} - \\mathbf{J}$ is symmetric idempotent such that\n",
    "\\begin{eqnarray*}\n",
    "  \\mathbb{E}(\\bar y_{\\cdot \\cdot}) &=& \\mathbb{E} \\left( \\frac{1}{na}\\mathbf{1}_{na}^T \\mathbf{y} \\right) = \\mu \\\\\n",
    "  \\mathbb{E} (\\text{SSE}) &=& \\mathbb{E} \\left[\\mathbf{y}^T (\\mathbf{I}_{na} - \\mathbf{P}) \\mathbf{y}\\right] = \\text{tr}\\left[(\\mathbf{I}_{na} - \\mathbf{P})(\\mathbf{I}_a \\otimes \\sigma_\\alpha^2\\mathbf{1}_{n}\\mathbf{1}_{n}^T + \\sigma_\\epsilon^2 \\mathbf{I}_{na})\\right] = a(n-1) \\sigma_{\\epsilon}^2\\\\\n",
    "  \\mathbb{E} (\\text{SSA}) &=& \\mathbb{E} \\left[\\mathbf{y}^T (\\mathbf{P} - \\mathbf{J}) \\mathbf{y}\\right] = \\text{tr}\\left[(\\mathbf{P} - \\mathbf{J})(\\mathbf{I}_a \\otimes \\sigma_\\alpha^2\\mathbf{1}_{n}\\mathbf{1}_{n}^T + \\sigma_\\epsilon^2 \\mathbf{I}_{na})\\right] = (a-1)(n \\sigma_{\\alpha}^2 + \\sigma_{\\epsilon}^2).\n",
    "\\end{eqnarray*}\n",
    "\n",
    "### MLE estimate\n",
    "\\begin{eqnarray*}\n",
    "\\mathbf{Y}_i &\\sim& N(\\boldsymbol{\\mu}, \\sigma_{\\alpha}^2 \\mathbf{1}_n \\mathbf{1}_n^T + \\sigma_{\\epsilon}^2 \\mathbf{I}_n) \\\\\n",
    "\\ell(\\mathbf{\\mu}, \\sigma_{\\alpha}^2, \\sigma_{\\epsilon}^2) &=& \\sum_{i=1}^a \\ell_i(\\mathbf{\\mu}, \\sigma_{\\alpha}^2, \\sigma_{\\epsilon}^2) \\\\\n",
    "&=& \\sum_{i=1}^a - \\frac{n}{2} \\log(2\\pi) - \\frac{1}{2} \\log \\det (\\sigma_{\\alpha}^2 \\mathbf{1}_n \\mathbf{1}_n^T + \\sigma_{\\epsilon}^2 \\mathbf{I}_n) - \\frac{1}{2} (\\mathbf{y}_i - \\boldsymbol{\\mu})^T (\\sigma_{\\alpha}^2 \\mathbf{1}_n \\mathbf{1}_n^T + \\sigma_{\\epsilon}^2 \\mathbf{I}_n)^{-1} (\\mathbf{y}_i - \\boldsymbol{\\mu}) \\\\\n",
    "(\\sigma_{\\alpha}^2 \\mathbf{1}_n \\mathbf{1}_n^T + \\sigma_{\\epsilon}^2 \\mathbf{I}_n)^{-1} &=& \\sigma_{\\epsilon}^{-2} \\mathbf{I}_n - \\frac{\\sigma_\\alpha^2}{\\sigma_\\epsilon^2 (\\sigma_\\epsilon^2 + n \\sigma_\\alpha^2)} \\mathbf{1}_n \\mathbf{1}_n^T \\quad (\\because \\text{SWB}) \\\\\n",
    "\\det (\\sigma_{\\alpha}^2 \\mathbf{1}_n \\mathbf{1}_n^T + \\sigma_{\\epsilon}^2 \\mathbf{I}_n) &=& \\det (\\sigma_{\\epsilon}^2 \\mathbf{I}_n) (1 + \\frac{n \\sigma_{\\alpha}^2}{\\sigma_{\\epsilon}^2}) = \\sigma_{\\epsilon}^{2n} (1 + \\frac{n \\sigma_{\\alpha}^2}{\\sigma_{\\epsilon}^2}) \\quad (\\because \\text{matrix determinant lemma}) \\\\ \n",
    "\\therefore \\ell(\\mathbf{\\mu}, \\sigma_{\\alpha}^2, \\sigma_{\\epsilon}^2) &=& \\sum_{i=1}^a - \\frac{n}{2} \\log(2\\pi) - \\frac{n}{2} \\log (\\sigma_{\\epsilon}^2) - \\frac{1}{2} \\log (1 + \\frac{n \\sigma_{\\alpha}^2}{\\sigma_{\\epsilon}^2}) - \\frac {1}{2 \\sigma_{\\epsilon}^2} (\\mathbf{y}_i - \\boldsymbol{\\mu})^T (\\mathbf{y}_i - \\boldsymbol{\\mu}) + \\frac {\\sigma_{\\alpha}^2}{2 \\sigma_{\\epsilon}^2 (\\sigma_{\\epsilon}^2 + n \\sigma_{\\alpha}^2)} n^2 (\\bar y_i - \\mu)^2 \\\\\n",
    "&=& - \\frac {na}{2} \\log(2\\pi) - \\frac {a(n-1)}{2} \\log (\\sigma_{\\epsilon}^2) - \\frac {a}{2} \\log (\\sigma_{\\epsilon}^2 + n \\sigma_{\\alpha}^2) - \\frac {1}{2 \\sigma_{\\epsilon}^2} (\\mathbf{y} - \\boldsymbol{\\mu})^T (\\mathbf{y} - \\boldsymbol{\\mu}) + \\frac {n \\sigma_{\\alpha}^2}{2 \\sigma_{\\epsilon}^2 (\\sigma_{\\epsilon}^2 + n \\sigma_{\\alpha}^2)} \\sum_{i=1}^a n (\\bar y_i - \\mu)^2 \\\\\n",
    "\\frac{\\partial \\ell}{\\partial \\mu} &=& \\sum_{i=1}^a \\sum_{j=1}^n \\frac{1}{\\sigma_{\\epsilon}^2} (y_{ij} - \\mu) - \\frac {n \\sigma_{\\alpha}^2}{\\sigma_{\\epsilon}^2 (\\sigma_{\\epsilon}^2 + n \\sigma_{\\alpha}^2)} \\sum_{i=1}^a n (\\bar y_i - \\mu) = 0 \\\\\n",
    "\\frac{\\partial \\ell}{\\partial \\sigma_{\\alpha}^2} &=& -\\frac{an}{2} \\frac {1}{(\\sigma_{\\epsilon}^2 + n \\sigma_{\\alpha}^2)} + \\frac{n \\text{SSA}}{2(\\sigma_{\\epsilon}^2 + n \\sigma_{\\alpha}^2)^2} = 0 \\\\\n",
    "\\frac{\\partial \\ell}{\\partial \\sigma_{\\epsilon}^2} &=& -\\frac{a(n - 1)}{2 \\sigma_{\\epsilon}^2} - \\frac {a}{2 (\\sigma_{\\epsilon}^2 + n \\sigma_{\\alpha}^2)} + \\frac{\\text{SST} - \\text{SSA}}{2 (\\sigma_{\\epsilon}^2)^2} + \\frac{\\text{SSA}}{2 (\\sigma_{\\epsilon}^2 + n \\sigma_{\\alpha}^2)^2} = 0\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\therefore \\hat{\\mu} &=& \\frac{\\sum_{ij} y_{ij}}{na}, \\\\\n",
    "\\widehat{\\sigma}_{\\alpha}^2 &=& \\frac{\\text{SSA}/a - \\widehat{\\sigma}_{\\epsilon}^2}{n}, \\\\\n",
    "\\widehat{\\sigma}_{\\epsilon}^2 &=& \\frac{\\text{SSE}}{a(n-1)}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "### REML estimate\n",
    "Suppose $\\mathbf{K} \\in \\mathbb{R}^{na \\times (na-1)}$ spans $\\mathcal{N}(\\boldsymbol\\mu^T)$. Then \n",
    "\\begin{eqnarray*}\n",
    "\\mathbf{K}^T \\mathbf{Y} &\\sim& N(\\mathbf{0}_{na-1}, \\sigma_{\\alpha}^2 \\mathbf{K}^T \\mathbf{Z} \\mathbf{Z}^T \\mathbf{K} + \\sigma_{\\epsilon}^2 \\mathbf{K}^T \\mathbf{K}) \\\\\n",
    "\\ell_{\\text{REML}}(\\mathbf{\\mu}, \\sigma_{\\alpha}^2, \\sigma_{\\epsilon}^2) &=& - \\frac{na-1}{2} \\log(2\\pi) - \\frac{1}{2} \\log \\det (\\sigma_{\\alpha}^2 \\mathbf{K}^T \\mathbf{Z} \\mathbf{Z}^T \\mathbf{K} + \\sigma_{\\epsilon}^2 \\mathbf{K}^T \\mathbf{K}) - \\frac{1}{2} (\\mathbf{K}^T \\mathbf{Y}_i)^T (\\sigma_{\\alpha}^2 \\mathbf{K}^T \\mathbf{Z} \\mathbf{Z}^T \\mathbf{K} + \\sigma_{\\epsilon}^2 \\mathbf{K}^T \\mathbf{K})^{-1} \\mathbf{K}^T \\mathbf{Y}_i.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Let $\\Omega = \\sigma_{\\alpha}^2 \\mathbf{K}^T \\mathbf{Z} \\mathbf{Z}^T \\mathbf{K} + \\sigma_{\\epsilon}^2 \\mathbf{K}^T \\mathbf{K}$. Then\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial \\ell_{\\text{REML}}}{\\partial \\sigma_{\\alpha}^2} &=& -\\frac{1}{2} \\text{tr}[\\Omega^{-1} \\mathbf{K}^T \\mathbf{Z} \\mathbf{Z}^T \\mathbf{K}] + \\frac{1}{2} \\mathbf{Y}^T \\mathbf{K} \\Omega^{-1} \\mathbf{K}^T \\mathbf{Z} \\mathbf{Z}^T\\mathbf{K} \\Omega^{-1} \\mathbf{K}^T \\mathbf{Y}  = 0 \\\\\n",
    "\\frac{\\partial \\ell_{\\text{REML}}}{\\partial \\sigma_{\\epsilon}^2} &=& -\\frac{1}{2} \\text{tr}[\\Omega^{-1} \\mathbf{K}^T \\mathbf{K}] + \\frac{1}{2} \\mathbf{Y}^T \\mathbf{K} \\Omega^{-1} \\mathbf{K}^T \\mathbf{K} \\Omega^{-1} \\mathbf{K}^T \\mathbf{Y}  = 0. \\\\\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Note $\\mathbf{K} \\Omega^{-1} \\mathbf{K}^T$ is invariant to choice of $\\mathbf{K}$, and it is equal to \n",
    "$$\n",
    "(\\sigma_{\\alpha}^2 \\mathbf{Z} \\mathbf{Z}^T + \\sigma_{\\epsilon}^2 \\mathbf{I}_{na})^{-1} - (\\sigma_{\\alpha}^2 \\mathbf{Z} \\mathbf{Z}^T + \\sigma_{\\epsilon}^2 \\mathbf{I}_{na})^{-1} \\mu \\mathbf{1}_{na}(\\mu \\mathbf{1}_{na}^T(\\sigma_{\\alpha}^2 \\mathbf{Z} \\mathbf{Z}^T + \\sigma_{\\epsilon}^2 \\mathbf{I}_{na})^{-1} \\mu \\mathbf{1}_{na})^{-1} \\mu \\mathbf{1}_{na}^T (\\sigma_{\\alpha}^2 \\mathbf{Z} \\mathbf{Z}^T + \\sigma_{\\epsilon}^2 \\mathbf{I}_{na})^{-1},\n",
    "$$\n",
    "where the following holds\n",
    "\\begin{eqnarray*}\n",
    "(\\sigma_{\\alpha}^2 \\mathbf{Z} \\mathbf{Z}^T + \\sigma_{\\epsilon}^2 \\mathbf{I}_{na})^{-1} &=& \\frac{1}{\\sigma_\\epsilon^2} \\mathbf{I}_{na} - \\frac{\\sigma_\\alpha^2}{\\sigma_\\epsilon^2(n \\sigma_{\\alpha}^2 + \\sigma_{\\epsilon}^2)} \\mathbf{Z} \\mathbf{Z}^T \\\\\n",
    "\\mathbf{Z}^T \\mathbf{Z} &=& n \\mathbf{I}_{a} \\\\\n",
    "\\mathbf{Z} \\mathbf{Z}^T &=& \\mathbf{I}_{a} \\otimes \\mathbf{1}_{n} \\mathbf{1}_{n}^T \\\\\n",
    "\\mathbf{Z}^T \\mathbf{1}_{na} &=& n \\mathbf{1}_{a} \\\\\n",
    "\\mathbf{Z} \\mathbf{Z}^T \\mathbf{1}_{na}  &=& n \\mathbf{1}_{na} \\\\ \n",
    "\\mathbf{Z} \\mathbf{Z}^T \\mathbf{1}_{na} \\mathbf{1}_{na}^T &=& n \\mathbf{1}_{na} \\mathbf{1}_{na}^T \\\\\n",
    "\\text{SSE} &=& \\mathbf{y}^T (\\mathbf{I}_{na} - \\frac{1}{n}\\mathbf{Z}\\mathbf{Z}^T) \\mathbf{y} \\\\\n",
    "\\text{SSA} &=& \\mathbf{y}^T (\\mathbf{I}_{na} - \\frac{1}{na}\\mathbf{1}_{na} \\mathbf{1}_{na}^T) \\mathbf{y}.\n",
    "\\end{eqnarray*}\n",
    "Then by algebra,\n",
    "\\begin{eqnarray*}\n",
    "\\mathbf{K} \\Omega^{-1} \\mathbf{K}^T &=& \\frac{1}{\\sigma_\\epsilon^2} \\mathbf{I}_{na} - \\frac{\\sigma_\\alpha^2}{\\sigma_\\epsilon^2(n \\sigma_{\\alpha}^2 + \\sigma_{\\epsilon}^2)} \\mathbf{Z} \\mathbf{Z}^T - \\frac{\\sigma_\\epsilon^2}{\\sigma_\\epsilon^2(n \\sigma_{\\alpha}^2 + \\sigma_{\\epsilon}^2)na} \\mathbf{1}_{na} \\mathbf{1}_{na}^T \\\\\n",
    "\\frac{na(n-1) \\sigma_{\\alpha}^2 + (na - 1)\\sigma_\\epsilon^2}{\\sigma_\\epsilon^2(n \\sigma_{\\alpha}^2 + \\sigma_{\\epsilon}^2)} &=& \\frac{[n^3 a^2 \\sigma_\\alpha^2 (n \\sigma_\\alpha^2 + 2 \\sigma_\\epsilon^2)] \\cdot \\text{SSE} + n^2 a^2 \\sigma_\\epsilon^4 \\cdot \\text{SSA}}{[\\sigma_\\epsilon^2(n \\sigma_{\\alpha}^2 + \\sigma_{\\epsilon}^2)na]^2}\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc0fc5b",
   "metadata": {},
   "source": [
    "The ANOVA estimate is essentially a method of moment estimate. The ANOVA and REML estimates are unbiased with $\\widehat{\\sigma}_{\\epsilon}^2  = \\frac{\\text{SSE}}{a(n-1)}$, $\\widehat{\\sigma}_\\alpha^2 = \\frac{\\text{SSA}}{(a-1)n} - \\frac{\\text{SSE}}{an(n-1)}$. For MLE, $\\widehat{\\sigma}_\\alpha^2$ is biased, but it might have a smaller mean squared error (MSE) than the ANOVA/REML estimate. \n",
    "This is analogous to the linear regression model $Y \\sim N(X\\beta, \\sigma^2 I)$, where the unbiased estimate of $\\sigma^2$ is $\\frac{\\text{SSE}}{n-p}$, the MLE is $\\frac{\\text{SSE}}{n}$, and the best quadratic invariant estimator with the smallest MSE is $\\frac{\\text{SSE}}{n-p + 2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4160029a",
   "metadata": {},
   "source": [
    "### Estimation of random effects\n",
    "\n",
    "Suppose the conditional distribution is\n",
    "$$\n",
    "\\mathbf{y} \\mid \\boldsymbol{\\gamma} \\sim N(\\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z} \\boldsymbol{\\gamma}, \\sigma^2 \\mathbf{I}_n)\n",
    "$$\n",
    "and the prior distribution\n",
    "$$\n",
    "\\boldsymbol{\\gamma} \\sim N(\\mathbf{0}_q, \\boldsymbol{\\Sigma}).\n",
    "$$\n",
    "Then by the Bayes theorem, the posterior distribution is\n",
    "\\begin{eqnarray*}\n",
    "f(\\boldsymbol{\\gamma} \\mid \\mathbf{y}) &=& \\frac{f(\\mathbf{y} \\mid \\boldsymbol{\\gamma}) \\times f(\\boldsymbol{\\gamma})}{f(\\mathbf{y})}, \\end{eqnarray*}\n",
    "where $f$ denotes corresponding density. Also the posterior distribution is a multivariate normal with mean\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\mathbb{E} (\\boldsymbol{\\gamma} \\mid \\mathbf{y}) &=& \\boldsymbol{\\Sigma} \\mathbf{Z}^T (\\mathbf{Z} \\boldsymbol{\\Sigma} \\mathbf{Z}^T + \\sigma^2 \\mathbf{I})^{-1} (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}) \\\\\n",
    "&=& (\\sigma^{-2} \\mathbf{Z}^T \\mathbf{Z} + \\boldsymbol{\\Sigma}^{-1})^{-1} \\mathbf{Z}^T \\sigma^{-2} (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}). \n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "For the balanced one-way ANOVA random effects model,\n",
    "\\begin{eqnarray*}\n",
    "\\mathbb{E} (\\boldsymbol{\\gamma} \\mid \\mathbf{y}) &=& (\\sigma_{\\epsilon}^{-2} \\mathbf{Z}^T \\mathbf{Z} + \\boldsymbol{\\Sigma}^{-1})^{-1} \\mathbf{Z}^T \\sigma_{\\epsilon}^{-2} (\\mathbf{y} - \\boldsymbol{\\mu}) \\\\\n",
    "&=& \\left[ n \\mathbf{I}_a + \\frac{\\sigma_{\\epsilon}^{2}}{\\sigma_{\\alpha}^{2}} \\mathbf{I}_a \\right]^{-1} \\mathbf{Z}^T (\\mathbf{y} - \\boldsymbol{\\mu}) = \\left( n + \\frac{\\sigma_{\\epsilon}^{2}}{\\sigma_{\\alpha}^{2}} \\right)^{-1} \\mathbf{Z}^T (\\mathbf{y} - \\boldsymbol{\\mu}) < (\\mathbf{Z}^T \\mathbf{Z})^{-1} \\mathbf{Z}^T (\\mathbf{y} - \\boldsymbol{\\mu}) = \\frac{1}{n} \\mathbf{Z}^T (\\mathbf{y} - \\boldsymbol{\\mu})\n",
    "\\end{eqnarray*}\n",
    "\n",
    "As such, the posterior mean of random effects is always a constant ($\\frac{n}{n + \\sigma_{\\epsilon}^{2} / \\sigma_{\\alpha}^{2}}$) dividing the corresponding fixed effects estimate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
