{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dirichlet-multinomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability mass function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In RNA-seq, for example, [count data](https://arxiv.org/abs/2001.04343) is commonly modeled using a Dirichlet-multinomial distribution, where the multinomial probabilities $\\mathbf{p} = (p_1,\\ldots, p_d)^T$ follow a Dirichlet distribution with the parameter vector $\\boldsymbol{\\alpha} = (\\alpha_1,\\ldots, \\alpha_d)^T$ and probability density function (pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\pi(\\mathbf{p}) =  \\frac{\\Gamma(|\\boldsymbol \\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\alpha_j>0$ and $|\\boldsymbol \\alpha|=\\sum_{j=1}^d \\alpha_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that $\\pi(\\mathbf{p})$ is a pdf, $\\int_{\\Delta_d} \\pi(\\mathbf{p}) d \\mathbf{p} = 1$ and hence $\\int_{\\Delta_d} \\prod_{j=1}^d p_j^{\\alpha_j-1} d \\mathbf{p} = \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j)}{\\Gamma(|\\boldsymbol \\alpha|)}$, where $\\Delta_d$ is the unit simplex in $d$ dimensions. Using this property, it is straightforward to show that $\\mathbb{E}[p_j] = \\frac{\\alpha_j}{|\\boldsymbol \\alpha|}$ and $\\mathbb{E}[p_j^2] = \\frac{\\alpha_j(\\alpha_j + 1)}{|\\boldsymbol \\alpha|(|\\boldsymbol \\alpha| + 1)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for a multivariate count vector $\\mathbf{x}=(x_1, \\ldots, x_d)^T$ with batch size $|\\mathbf{x}|=\\sum_{j=1}^d x_j$, the probability mass function (pmf) for Dirichlet-multinomial distribution is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(\\mathbf{x} \\mid \\boldsymbol \\alpha) = \\int_{\\Delta_d} f(\\mathbf{x} \\mid \\mathbf{p}, \\boldsymbol \\alpha) \\cdot \\pi(\\mathbf{p}) d \\mathbf{p}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\int_{\\Delta_d} \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j} \\cdot \\pi(\\mathbf{p}) \\, d \\mathbf{p}  \n",
    "= \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_j)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\Gamma(|\\boldsymbol \\alpha|)}{\\Gamma(|\\boldsymbol \\alpha|+|\\mathbf{x}|)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given independent data points $\\mathbf{x}_1, \\ldots, \\mathbf{x}_n$, the log-likelihood is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(\\boldsymbol \\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\boldsymbol \\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\boldsymbol \\alpha|)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\boldsymbol \\alpha|+k).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last equality holds, since $\\frac{\\Gamma(a + k)}{\\Gamma(a)} = a (a + 1) \\cdots (a+k-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient and Hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $\\frac{\\partial}{\\partial x} \\ln \\Gamma(x) = \\frac{\\Gamma'(x)}{\\Gamma(x)} = \\Psi(x)$ and $\\frac{\\partial^2}{\\partial x^2} \\ln \\Gamma(x) = \\Psi'(x)$, the score function is $\\nabla L(\\boldsymbol \\alpha) = (\\text{D} L(\\boldsymbol \\alpha))^T$, where "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial}{\\partial \\alpha_j} L(\\boldsymbol \\alpha) = \\sum_{i=1}^n [\\Psi(\\alpha_j + x_{ij}) - \\Psi(\\alpha_j)] - \\sum_{i=1}^n [\\Psi(|\\boldsymbol \\alpha|+|\\mathbf{x}_i|) - \\Psi(|\\boldsymbol \\alpha|)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\sum_{i=1}^n \\sum_{k=0}^{x_{ij}-1} \\frac{1}{\\alpha_j+k} - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\frac{1}{|\\boldsymbol \\alpha|+k}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observed information is $-\\nabla^2L(\\alpha) = - \\text{D} (\\text{D} L(\\boldsymbol \\alpha))^T$, where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "-\\frac{\\partial^2}{\\partial \\alpha_j \\partial \\alpha_l} L(\\boldsymbol \\alpha) = \n",
    "\\begin{cases}\n",
    "- \\sum_{i=1}^n [\\Psi'(\\alpha_j + x_{ij}) - \\Psi'(\\alpha_j)] + \\sum_{i=1}^n [\\Psi'(|\\boldsymbol \\alpha|+|\\mathbf{x}_i|) - \\Psi'(|\\boldsymbol \\alpha|)], & l = j \\\\\n",
    "\\sum_{i=1}^n [\\Psi'(|\\boldsymbol \\alpha|+|\\mathbf{x}_i|) - \\Psi'(|\\boldsymbol \\alpha|)], & l \\neq j\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \n",
    "\\begin{cases}\n",
    "\\sum_{i=1}^n \\sum_{k=0}^{x_{ij}-1} \\frac{1}{(\\alpha_j+k)^2} - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\frac{1}{(|\\boldsymbol \\alpha|+k)^2}, & l = j \\\\\n",
    "- \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\frac{1}{(|\\boldsymbol \\alpha|+k)^2}, & l \\neq j\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the log-likelihood is not a concave function, since $-\\frac{\\partial^2}{\\partial \\alpha_1^2} L(\\alpha) = - \\sum_{i=1}^n [\\Psi'(\\alpha_1 + x_{i1}) - \\Psi'(\\alpha_1)] + \\sum_{i=1}^n [\\Psi'(|\\boldsymbol \\alpha|+|\\mathbf{x}_i|) - \\Psi'(|\\boldsymbol \\alpha|)]$ could be negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected Fisher information is $\\mathbb{E}[-\\nabla^2L(\\boldsymbol \\alpha)]$, where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}\\left[-\\frac{\\partial^2}{\\partial \\alpha_j \\partial \\alpha_l} L(\\boldsymbol \\alpha)\\right] = \n",
    "\\begin{cases}\n",
    "\\sum_{i=1}^n \\sum_{x_{ij}=0}^{|\\boldsymbol{x_i}|} \\sum_{k=0}^{x_{ij}-1} \\frac{1}{(\\alpha_j+k)^2} f(x_{ij}) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\frac{1}{(|\\boldsymbol \\alpha|+k)^2}, & l = j \\\\\n",
    "- \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\frac{1}{(|\\boldsymbol \\alpha|+k)^2}, & l \\neq j\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $f(x_{ij})$ is the marginal distribution of $f(\\boldsymbol{x})$ for $x_{ij}$. Here, Fisher scoring method is inefficient for computing maximum likelihood estimate (MLE), since calculation of the expected information matrix is difficult. So we instead use a positive definite matrix that is approximated from the observed information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $(p_1,\\ldots,p_d) \\in \\Delta_d = \\{\\mathbf{p}: p_i \\ge 0, \\sum_i p_i = 1\\}$ follows a Dirichlet distribution with parameter $\\boldsymbol \\alpha = (\\alpha_1,\\ldots,\\alpha_d)$. Then taking the derivative with respect to $\\alpha_k$ on both sides, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\int_{\\Delta_d}\\frac{\\Gamma(|\\boldsymbol \\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p} = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\rightarrow \\frac{\\partial}{\\partial \\alpha_k}\\int_{\\Delta_d}\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p} = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\int_{\\Delta_d}\\bigg(\\frac{\\Gamma'(|\\boldsymbol \\alpha|)\\prod_{j=1}^{d}\\Gamma(\\alpha_j)-\\Gamma(|\\boldsymbol \\alpha|)\\Gamma'(\\alpha_k)\\prod_{j\\neq k}^{d}\\Gamma(\\alpha_j)}{\\prod_{j=1}^{d}\\Gamma(\\alpha_j)^{2}}\\bigg)\\prod_{j=1}^d p_j^{\\alpha_j-1}+\\frac{\\Gamma(|\\boldsymbol \\alpha|)}{\\prod_{j=1}^{d}\\Gamma(\\alpha_j)}\\ln(p_k)\\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}=0\\\\\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\therefore \\mathbb{E}\\left[\\ln(p_k)\\right] = \\int_{\\Delta_d}\\bigg(\\frac{\\Gamma(|\\boldsymbol \\alpha|)\\Gamma'(\\alpha_k)\\prod_{j\\neq k}^{d}\\Gamma(\\alpha_j)-\\Gamma'(|\\boldsymbol \\alpha|)\\prod_{j=1}^{d}\\Gamma(\\alpha_j)}{\\prod_{j=1}^{d}\\Gamma(\\alpha_j)^{2}}\\bigg)\\prod_{j=1}^d p_j^{\\alpha_j-1}d\\mathbf{p}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\frac{\\Gamma'(\\alpha_k)}{\\Gamma(\\alpha_k)} - \\frac{\\Gamma'(|\\boldsymbol \\alpha|)}{\\Gamma(|\\boldsymbol \\alpha|)} \n",
    "= \\Psi(\\alpha_k) - \\Psi(|\\boldsymbol \\alpha|).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate Hessian matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observed information matrix is not positive definite as mentioned above, but it takes on the Woodbury form, which we can take advantage of to approximate a positive definite matrix. Specifically, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(\\mathbf{A} + c \\cdot \\boldsymbol{u} \\boldsymbol{u}^T)^{-1} = \\mathbf{A}^{-1} - c \\cdot \\frac{1}{1 + c \\cdot \\boldsymbol{u}^T \\mathbf{A}^{-1} \\boldsymbol{u}} \\mathbf{A}^{-1} \\boldsymbol{u} \\boldsymbol{u}^T \\mathbf{A}^{-1},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{A}$ is a diagonal matrix with positive entries, $c$ is a negative constant. Since $\\mathbf{A} + c \\cdot \\boldsymbol{u} \\boldsymbol{u}^T$ is positive definite if and only if $(\\mathbf{A} + c \\cdot \\boldsymbol{u} \\boldsymbol{u}^T)^{-1}$ is positive definite, if we let $c = -0.95 \\cdot (\\boldsymbol{u}^T \\mathbf{A}^{-1} \\boldsymbol{u})^{-1}$ whenever $1 + c \\cdot \\boldsymbol{u}^T \\mathbf{A}^{-1} \\boldsymbol{u} < 0$, we can guarantee the positive definiteness of the Hessian matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following (quasi) method of moment estimator for $\\boldsymbol{\\alpha}$ would be a good starting point for iterative algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\alpha_j = \\frac{\\mathbb{E}[p_j]^2 - \\mathbb{E}[p_j]\\mathbb{E}[p_j^2]}{\\mathbb{E}[p_j^2] - \\mathbb{E}[p_j]^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Newton's algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SpecialFunctions, LinearAlgebra\n",
    "polygamma(0, 0.5)  # digamma(0.5)\n",
    "polygamma(1, 0.5);  # trigamma(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_logpdf"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_logpdf(x::Vector, α::Vector)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at data point `x`.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(x::Vector, α::Vector)\n",
    "    xsum = sum(x)\n",
    "    αsum = sum(α)\n",
    "    loglike = logfactorial(xsum)\n",
    "    for j in 1:length(x)\n",
    "        loglike = loglike - logfactorial(x[j]) + loggamma(α[j] + x[j]) - loggamma(α[j])\n",
    "    end\n",
    "    loglike = loglike - loggamma(xsum + αsum) + loggamma(αsum)\n",
    "    return loglike\n",
    "end\n",
    "\n",
    "function dirmult_logpdf!(r::Vector, X::Matrix, α::Vector)\n",
    "    for i in 1:size(X, 2)\n",
    "        r[i] = dirmult_logpdf(X[:, i], α)\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    dirmult_logpdf(X, α)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at each data point in `X`. Each column of `X` is one data point.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(X::Matrix, α::Vector)\n",
    "    r = zeros(size(X, 2))\n",
    "    dirmult_logpdf!(r, X, α)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_newton"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_newton(X)\n",
    "\n",
    "Find the MLE of Dirichlet-multinomial distribution using Newton's method.\n",
    "\n",
    "# Argument\n",
    "* `X`: an `d`-by-`n` matrix of counts; each column is one data point.\n",
    "\n",
    "# Optional argument  \n",
    "* `α0`: a `d` vector of starting point (optional). \n",
    "* `maxiters`: the maximum allowable Newton iterations (default 100). \n",
    "* `tolfun`: the tolerance for  relative change in objective values (default 1e-6). \n",
    "\n",
    "# Output\n",
    "* `maximum`: the log-likelihood at MLE.   \n",
    "* `estimate`: the MLE. \n",
    "* `gradient`: the gradient at MLE. \n",
    "* `hessian`: the Hessian at MLE. \n",
    "* `se`: a `d` vector of standard errors. \n",
    "* `iterations`: the number of iterations performed.\n",
    "\"\"\"\n",
    "function dirmult_newton(X::Matrix; α0::Union{Vector, Nothing} = nothing, \n",
    "            maxiters::Int = 100, tolfun::Float64 = 1e-6)\n",
    "    # set default starting point as method of moment estimates\n",
    "    d, n = size(X)\n",
    "    tot = sum(X, dims = 1)\n",
    "    if α0 == nothing\n",
    "        p = X * Diagonal(1 ./ tot)\n",
    "        Ep = sum(p, dims = 2) / n\n",
    "        Ep² = sum(p.^2, dims = 2) / n\n",
    "        α0 = vec((Ep.^2 - Ep .* Ep²) ./ (Ep² - Ep.^2))\n",
    "    end\n",
    "    # calculate initial log-likelihood\n",
    "    loglold = sum(dirmult_logpdf(X, α0))\n",
    "    # initialize arrays\n",
    "    ∇α = zeros(d)\n",
    "    ∇²α = ones(d, d)\n",
    "    dir = zeros(d, d)\n",
    "    A = zeros(d)\n",
    "    A⁻¹ = zeros(d)\n",
    "    c = 0\n",
    "    α = copy(α0)\n",
    "    logl = loglold\n",
    "    # Newton loop\n",
    "    iter = 1\n",
    "    for outer iter in 1:maxiters\n",
    "        # evaluate gradient (score)\n",
    "        α0sum = sum(α0)\n",
    "        for j in 1:d\n",
    "            for i in 1:n\n",
    "                ∇α[j] += polygamma(0, X[j, i] + α0[j]) - polygamma(0, α0[j]) -\n",
    "                    polygamma(0, tot[i] + α0sum) + polygamma(0, α0sum)\n",
    "            end\n",
    "        end\n",
    "        # compute Newton's direction\n",
    "        for j in 1:d\n",
    "            for i in 1:n \n",
    "                A[j] -= polygamma(1, X[j, i] +  α0[j]) - polygamma(1, α0[j])\n",
    "                c += polygamma(1, tot[i] + α0sum) - polygamma(1, α0sum)\n",
    "            end\n",
    "            A⁻¹[j] = 1 / A[j]\n",
    "        end\n",
    "        # compute Hessian\n",
    "        ∇²α .= c .* ∇²α\n",
    "        for j in 1:d\n",
    "            ∇²α[j, j] += A[j]\n",
    "        end\n",
    "        if (1 + c * sum(A⁻¹)) <= 0\n",
    "            c = -0.95 / sum(A⁻¹)\n",
    "        end\n",
    "        BLAS.ger!(-c / (1 + c * sum(A⁻¹)), A⁻¹, A⁻¹, dir)\n",
    "        for j in 1:d\n",
    "            dir[j, j] += A⁻¹[j]\n",
    "        end\n",
    "        # line search loop\n",
    "        for lsiter in 1:20\n",
    "            # step halving\n",
    "            s = 2.0^(1 - lsiter)\n",
    "            copy!(α, α0)\n",
    "            BLAS.gemv!('N', s, dir, ∇α, 1.0, α)\n",
    "            if (minimum(α) > 0)\n",
    "                logl = sum(dirmult_logpdf(X, α))\n",
    "                if (logl > loglold)\n",
    "                    break\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        # check convergence criterion\n",
    "        if abs(logl - loglold) < tolfun * (abs(loglold) + 1)\n",
    "            break\n",
    "        end\n",
    "        copy!(∇α, zeros(d))\n",
    "        copy!(∇²α, ones(d, d))\n",
    "        copy!(dir, zeros(d, d))\n",
    "        copy!(A, zeros(d))\n",
    "        c = 0\n",
    "        copy!(α0, α)\n",
    "        loglold = logl\n",
    "    end\n",
    "    # output\n",
    "    return logl, α, ∇α, ∇²α, iter\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load example data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we build a classifer for handwritten digit recognition. Following figure shows example bitmaps of handwritten digits from U.S. postal envelopes. \n",
    "\n",
    "<img src=\"./handwritten_digits.png\" width=\"250\" align=\"center\"/>\n",
    "\n",
    "Each digit is represented by a $32 \\times 32$ bitmap in which each element indicates one pixel with a value of white or black. Each $32 \\times 32$ bitmap is divided into blocks of $4 \\times 4$, and the number of white pixels are counted in each block. Therefore each handwritten digit is summarized by a vector $\\mathbf{x} = (x_1, \\ldots, x_{64})$ of length 64 where each element is a count between 0 and 16. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3823, 65)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using DelimitedFiles\n",
    "\n",
    "optdigits = readdlm(\"./optdigits.tra\", ',', Int64)\n",
    "size(optdigits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = copy(transpose(optdigits[:, 1:64]))\n",
    "digits = optdigits[:, 65];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data consists of 3,823 handwritten digits. Each row contains the 64 counts of a digit and the last element (65th element) indicates what digit it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirmult_logpdf(data, ones(size(data, 1)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE for example data \n",
    "We can estimate the MLE for digit 0, digit 1, ..., and digit 9 separately, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64×10 Matrix{Float64}:\n",
       "  0.0        0.0         0.0         …  0.0         0.0         0.0\n",
       "  0.0374846  0.00802891  0.387042       0.140215    0.0880215   0.0764813\n",
       "  4.99914    0.514275    3.7928         2.51112     2.49913     1.53133\n",
       " 14.9145     2.09965     5.21171        5.089       5.83687     3.70784\n",
       " 12.1794     3.05823     2.49522        5.48423     5.58558     3.74053\n",
       "  2.45887    1.3218      0.30935     …  4.48988     2.42446     1.51447\n",
       "  0.0635862  0.137833    0.0157198      1.73032     0.199805    0.298736\n",
       "  0.0        0.0         0.0            0.179722    0.0         0.0159526\n",
       "  0.0        0.0         0.0            0.0         0.00219868  0.0\n",
       "  1.02489    0.0573021   1.72338        0.255966    1.02342     0.667389\n",
       " 14.6158     1.04117     5.46516     …  3.19455     6.1076      4.02737\n",
       " 14.7766     3.24159     4.72352        4.01438     4.56622     3.35465\n",
       " 13.7623     4.1066      4.41509        4.19522     4.08925     3.2949\n",
       "  ⋮                                  ⋱                          \n",
       " 11.742      4.05117     4.36878        1.56647     3.34793     2.22513\n",
       " 15.259      2.27414     3.81897        0.0533557   4.03751     2.62179\n",
       "  2.59429    0.272957    2.35309        0.00192176  0.776827    0.985207\n",
       "  0.0        0.0303369   0.135627    …  0.0         0.0         0.0159913\n",
       "  0.0        0.0         0.00192946     0.0         0.0         0.0\n",
       "  0.0207856  0.0177473   0.353515       0.09963     0.0780352   0.0536034\n",
       "  5.03247    0.514841    3.86303        2.85568     2.57553     1.4669\n",
       " 15.3962     2.01984     5.23182        4.14504     6.17371     3.77095\n",
       " 15.0941     3.52434     4.90672     …  0.453626    5.71606     3.56354\n",
       "  5.83072    2.20929     4.49282        0.0118088   2.69269     1.61729\n",
       "  0.194619   0.408502    3.00212        0.0         0.300055    0.335896\n",
       "  0.0        0.04194     0.333333       0.0         0.00660675  0.00702424"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = zeros(size(data)[1], 10)\n",
    "for digit in 0:9\n",
    "    X = data[:, digits .== digit]\n",
    "    ind = zeros(Int64, size(findall(!iszero, sum(X, dims = 2))))\n",
    "    for i in 1:size(ind)[1]\n",
    "        ind[i] = findall(!iszero, sum(X, dims = 2))[i][1]\n",
    "    end\n",
    "    X = X[ind, :]\n",
    "    out[ind, digit + 1] = dirmult_newton(X)[2]\n",
    "end\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with multinomial distribution \n",
    "\n",
    "As $|\\alpha| \\to \\infty$ and $\\alpha / |\\alpha| \\to \\mathbf{p}$, the Dirichlet-multinomial distribution converges to a multinomial with parameter $\\mathbf{p}$. Therefore multinomial can be considered as a special case of Dirichlet-multinomial with $|\\alpha|=\\infty$. We perform a likelihood ratio test (LRT) whether Dirichlet-multinomial offers a better fit than multinomial for digits 0, 1, ..., 9 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mult_logpdf"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    mult_logpdf(x::Vector, p::Vector)\n",
    "    \n",
    "Compute the log-pdf of multinomial distribution with parameter `p` \n",
    "at data point `x`.\n",
    "\"\"\"\n",
    "function mult_logpdf(x::Vector, p::Vector)\n",
    "    xsum = sum(x)\n",
    "    loglike = logfactorial(xsum)\n",
    "    for j in 1:length(x)\n",
    "        loglike = loglike - logfactorial(x[j])\n",
    "    end\n",
    "    loglike = loglike + dot(x, log.(p))\n",
    "    return loglike\n",
    "end\n",
    "\n",
    "function mult_logpdf!(r::Vector, X::Matrix, p::Vector)\n",
    "    for i in 1:size(X, 2)\n",
    "        r[i] = mult_logpdf(X[:, i], p)\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    mult_logpdf(X, p)\n",
    "    \n",
    "Compute the log-pdf of multinomial distribution with parameter `p` \n",
    "at each data point in `X`. Each column of `X` is one data point.\n",
    "\"\"\"\n",
    "function mult_logpdf(X::Matrix, p::Vector)\n",
    "    r = zeros(size(X, 2))\n",
    "    mult_logpdf!(r, X, p)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×2 Matrix{Float64}:\n",
       " -37358.4  -39592.2\n",
       " -42179.3  -54039.2\n",
       " -39985.3  -49111.5\n",
       " -40519.5  -47089.1\n",
       " -43488.8  -57344.1\n",
       " -41191.3  -51713.0\n",
       " -37702.5  -42597.3\n",
       " -40304.1  -49473.0\n",
       " -43130.8  -49695.9\n",
       " -43709.7  -54577.8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loglikeout = zeros(10, 2)\n",
    "for digit in 0:9\n",
    "    X = data[:, digits .== digit]\n",
    "    ind = zeros(Int64, size(findall(!iszero, sum(X, dims = 2))))\n",
    "    for i in 1:size(ind)[1]\n",
    "        ind[i] = findall(!iszero, sum(X, dims = 2))[i][1]\n",
    "    end\n",
    "    X = X[ind, :]\n",
    "    loglikeout[digit + 1, 1] = dirmult_newton(X)[1]\n",
    "    p = vec(sum(X, dims = 2)) / sum(X)\n",
    "    loglikeout[digit + 1, 2] = sum(mult_logpdf(X, p))\n",
    "end\n",
    "loglikeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Float64}:\n",
       "  4467.550000359493\n",
       " 23719.94312609102\n",
       " 18252.218848676377\n",
       " 13139.208044741405\n",
       " 27710.49772595527\n",
       " 21043.37302165093\n",
       "  9789.588765652516\n",
       " 18337.851525632417\n",
       " 13130.003409554833\n",
       " 21736.264647779986"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teststat = 2 * (loglikeout[:, 1] - loglikeout[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification in an independent dataset\n",
    "\n",
    "Now we construct a simple Bayesian rule for handwritten digits recognition:\n",
    "$$\n",
    "\t\\mathbf{x}\t\\mapsto \\arg \\max_k \\widehat \\pi_k f(x|\\widehat \\alpha_k).\n",
    "$$\n",
    "Here we can use the proportion of digit $k$ in the training set as the prior probability $\\widehat \\pi_k$. We report the performance of our classifier on a test set of 1797 digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 65)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = readdlm(\"./optdigits.tes\", ',', Int64)\n",
    "size(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = copy(transpose(test[:, 1:64]))\n",
    "testdigits = test[:, 65];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8747913188647746"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior = zeros(10)\n",
    "predict = zeros(size(testdata)[2], 11)\n",
    "for digit in 0:9\n",
    "    prior[digit + 1] = sum(digits .== digit) / length(digits)\n",
    "    ind = findall(!iszero, out[:, digit + 1])\n",
    "    predict[:, digit + 1] = dirmult_logpdf(testdata[ind, :], out[ind, digit + 1]) .+ log(prior[digit + 1])\n",
    "end\n",
    "\n",
    "for i in 1:size(predict)[1]\n",
    "    predict[i, end] = findmax(predict[i, 1:10])[2] - 1\n",
    "end\n",
    "\n",
    "sum(testdigits .== predict[:, end]) / size(predict)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.6.0\n",
      "Commit f9720dc2eb (2021-03-24 12:55 UTC)\n",
      "Platform Info:\n",
      "  OS: macOS (x86_64-apple-darwin19.6.0)\n",
      "  CPU: Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-11.0.1 (ORCJIT, skylake)\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "65px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
